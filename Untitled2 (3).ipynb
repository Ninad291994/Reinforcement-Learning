{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpbmtaXp0EeR"
   },
   "source": [
    "Maze Solver using e-greedy with realistic vs optimal initialization of Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Hw17lqTU0dIq",
    "outputId": "c739a34a-de95-419a-85a5-b7e66cb277c9"
   },
   "outputs": [],
   "source": [
    "# install required system dependencies\n",
    "!apt-get install -y xvfb x11-utils  \n",
    "!apt-get install x11-utils > /dev/null 2>&1\n",
    "!pip install PyOpenGL==3.1.* \\\n",
    "            PyOpenGL-accelerate==3.1.* \\\n",
    "            gym[box2d]==0.17.* \n",
    "!pip install pyglet\n",
    "!pip install ffmpeg\n",
    "! pip install pyvirtualdisplay\n",
    "!pip install Image\n",
    "!pip install gym-maze-trustycoder83\n",
    "#!pip install plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRSPGz141UDD",
    "outputId": "1150aaa0-1850-469e-8733-2fec9854139c"
   },
   "outputs": [],
   "source": [
    "!mkdir ./vid\n",
    "!rm ./vid/*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiuOXX6i1cUR",
    "outputId": "4489dbdc-1e5e-419a-d81a-828d7736bf55"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# import pygame\n",
    "import numpy as np\n",
    "# import math\n",
    "# import base64\n",
    "# import io\n",
    "# import IPython\n",
    "import gym\n",
    "import gym_maze\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "# from gym.wrappers import Monitor\n",
    "# from IPython import display\n",
    "#from pyvirtualdisplay import Display\n",
    "#from gym.wrappers.monitoring import video_recorder\n",
    "\n",
    "#d = Display()\n",
    "#d.start()\n",
    "\n",
    "# Recording filename\n",
    "#video_name = \"./vid/Practical_2.mp4\"\n",
    "\n",
    "# Setup the environment for the maze\n",
    "env = gym.make(\"maze-sample-10x10-v0\")\n",
    "\n",
    "# Setup the video\n",
    "#vid = None\n",
    "#vid = video_recorder.VideoRecorder(env,video_name)\n",
    "\n",
    "# env = gym.wrappers.Monitor(env,'./vid',force=True)\n",
    "current_state = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyUxTsy55QCD"
   },
   "source": [
    "1. Realistic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzKbEmdx5I8V",
    "outputId": "3d53ae00-6bc6-410b-bfac-5e7f569bd521"
   },
   "outputs": [],
   "source": [
    "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        states_dic[i, j] = count\n",
    "        count+=1\n",
    "        \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize the Q-table to 0\n",
    "Q_table_1 = {}\n",
    "\n",
    "\n",
    "# Number of episode we will run\n",
    "n_episodes = 300\n",
    "\n",
    "\n",
    "# Initialize the exploration probability to 1\n",
    "exploration_proba = [0.2, 0.4, 0.8]\n",
    "gamma_list = [0.1, 0.4, 0.8]\n",
    "\n",
    "alpha = 1 / n_actions\n",
    "\n",
    "\n",
    "rewards_per_episode_dict = {}\n",
    "\n",
    "for gamma in gamma_list:\n",
    "      for proba in exploration_proba:\n",
    "        rewards_per_episode_dict['{},{}'.format('%f'%proba, '%f'%gamma)] = []\n",
    "        Q_table = * np.ones((len(states_dic),n_actions))\n",
    "\n",
    "        # Iterate over episodes\n",
    "        for e in range(n_episodes):\n",
    "\n",
    "            # We are not done yet\n",
    "            done = False\n",
    "            steps = 0\n",
    "            # Sum the rewards that the agent gets from the environment\n",
    "            total_episode_reward = 0\n",
    "\n",
    "            for i in itertools.count(): \n",
    "                env.unwrapped.render()\n",
    "                #vid.capture_frame()\n",
    "                current_coordinate_x = int(current_state[0])\n",
    "                current_coordinate_y = int(current_state[1])\n",
    "                current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
    "\n",
    "                if np.random.uniform(0,1) < proba:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
    "                next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
    "\n",
    "\n",
    "                # Update our Q-table using the Q-learning iteration\n",
    "                next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
    "                Q_table[current_Q_table_coordinates, action] = Q_table[current_Q_table_coordinates, action] + (alpha * (reward + (gamma * ( Q_table[next_Q_table_coordinates, action])) - Q_table[current_Q_table_coordinates, action]))\n",
    "                #Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + max(Q_table[next_Q_table_coordinates,:]))\n",
    "\n",
    "                total_episode_reward = total_episode_reward + reward\n",
    "\n",
    "\n",
    "                #step.append(e)\n",
    "                #reward_.append(reward)\n",
    "\n",
    "                steps = i\n",
    "\n",
    "\n",
    "\n",
    "                #if e == 9:\n",
    "                  #avg_reward = total_episode_reward / (i+1)\n",
    "                  #dict_step_avgReward[i+1] = total_episode_reward\n",
    "\n",
    "                # If the episode is finished, we leave the for loop\n",
    "                if done:\n",
    "                    break\n",
    "                current_state = next_state\n",
    "\n",
    "            #Show the total episode reward        \n",
    "            print(\"Total episode reward:\", total_episode_reward)\n",
    "\n",
    "            #Reset enviroment for next episode\n",
    "            current_state = env.reset()\n",
    "\n",
    "            rewards_per_episode_dict['{},{}'.format('%f'%proba, '%f'%gamma)].append(total_episode_reward / steps)\n",
    "        Q_table_1['{},{}'.format('%f'%proba, '%f'%gamma)] = Q_table\n",
    "    # Save video episode and close\n",
    "#print(\"Video successfuly saved.\")\n",
    "#vid.close()\n",
    "#vid.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCOrwneDTZIK"
   },
   "source": [
    "import base64\n",
    "import io\n",
    "from IPython import display\n",
    "\n",
    "video_name = \"./vid/Practical_2.mp4\"\n",
    "\n",
    "video = io.open(video_name, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "\n",
    "display.display(display.HTML(data=\"\"\"\n",
    "  <video alt=\"test\" controls>\n",
    "  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "  </video>\n",
    "  \"\"\".format(encoded.decode('ascii'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('q.pkl', 'wb') as f:\n",
    "    pickle.dump(Q_table_1, f)\n",
    "    \n",
    "with open('r.pkl', 'wb') as f:\n",
    "    pickle.dump(rewards_per_episode_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "mEYOwiopzbsD",
    "outputId": "fb32350b-bf20-4f45-b33d-2b3487463e59"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in rewards_per_episode_dict.keys():\n",
    "    y = savgol_filter(rewards_per_episode_dict[i], 51, 1)\n",
    "    plt.plot(range(1,len(rewards_per_episode_dict[i])+1), y, label='%s'%i)\n",
    "    \n",
    "    \n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Q_table_1['0.200000,0.800000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHhXXz5xDJw3"
   },
   "source": [
    "2. Optimal Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tdV7uzxDMaS"
   },
   "outputs": [],
   "source": [
    "states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        states_dic[i, j] = count\n",
    "        count+=1\n",
    "        \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize the Q-table to 0\n",
    "Q_table_2 = {}\n",
    "\n",
    "\n",
    "# Number of episode we will run\n",
    "n_episodes = 300\n",
    "\n",
    "\n",
    "# Initialize the exploration probability to 1\n",
    "exploration_proba = [0.2, 0.4, 0.8]\n",
    "gamma_list = [0.1, 0.4, 0.8]\n",
    "\n",
    "alpha = 1 / n_actions\n",
    "\n",
    "\n",
    "rewards_per_episode_dict_1 = {}\n",
    "\n",
    "for gamma in gamma_list:\n",
    "      for proba in exploration_proba:\n",
    "        rewards_per_episode_dict_1['{},{}'.format('%f'%proba, '%f'%gamma)] = []\n",
    "        Q_table =  1.6420482407868309 * np.ones((len(states_dic),n_actions))\n",
    "\n",
    "        # Iterate over episodes\n",
    "        for e in range(n_episodes):\n",
    "\n",
    "            # We are not done yet\n",
    "            done = False\n",
    "            steps = 0\n",
    "            # Sum the rewards that the agent gets from the environment\n",
    "            total_episode_reward = 0\n",
    "\n",
    "            for i in itertools.count(): \n",
    "                env.unwrapped.render()\n",
    "                #vid.capture_frame()\n",
    "                current_coordinate_x = int(current_state[0])\n",
    "                current_coordinate_y = int(current_state[1])\n",
    "                current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
    "\n",
    "                if np.random.uniform(0,1) < proba:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
    "                next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
    "\n",
    "\n",
    "                # Update our Q-table using the Q-learning iteration\n",
    "                next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
    "                Q_table[current_Q_table_coordinates, action] = Q_table[current_Q_table_coordinates, action] + (alpha * (reward + (gamma * ( Q_table[next_Q_table_coordinates, action])) - Q_table[current_Q_table_coordinates, action]))\n",
    "                #Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + max(Q_table[next_Q_table_coordinates,:]))\n",
    "\n",
    "                total_episode_reward = total_episode_reward + reward\n",
    "\n",
    "\n",
    "                #step.append(e)\n",
    "                #reward_.append(reward)\n",
    "\n",
    "                steps = i\n",
    "\n",
    "\n",
    "\n",
    "                #if e == 9:\n",
    "                  #avg_reward = total_episode_reward / (i+1)\n",
    "                  #dict_step_avgReward[i+1] = total_episode_reward\n",
    "\n",
    "                # If the episode is finished, we leave the for loop\n",
    "                if done:\n",
    "                    break\n",
    "                current_state = next_state\n",
    "\n",
    "            #Show the total episode reward        \n",
    "            print(\"Total episode reward:\", total_episode_reward)\n",
    "\n",
    "            #Reset enviroment for next episode\n",
    "            current_state = env.reset()\n",
    "\n",
    "            rewards_per_episode_dict_1['{},{}'.format('%f'%proba, '%f'%gamma)].append(total_episode_reward / steps)\n",
    "        Q_table_2['{},{}'.format('%f'%proba, '%f'%gamma)] = Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q1.pkl', 'wb') as f:\n",
    "    pickle.dump(Q_table_2, f)\n",
    "    \n",
    "with open('r1.pkl', 'wb') as f:\n",
    "    pickle.dump(rewards_per_episode_dict_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in rewards_per_episode_dict_1.keys():\n",
    "    y = savgol_filter(rewards_per_episode_dict_1[i], 51, 1)\n",
    "    plt.plot(range(1,len(rewards_per_episode_dict_1[i])+1), y, label='%s'%i)\n",
    "    \n",
    "    \n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
