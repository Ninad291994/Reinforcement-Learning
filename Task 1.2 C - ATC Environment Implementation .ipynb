{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.18.0.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from gym) (1.19.2)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting Pillow<=7.2.0\n",
      "  Downloading Pillow-7.2.0-cp36-cp36m-win_amd64.whl (2.0 MB)\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656449 sha256=a6fc9cd1f70724551060ee228b474e1af7bda02ae04e871d3d051ed80703ef5d\n",
      "  Stored in directory: c:\\users\\61431\\appdata\\local\\pip\\cache\\wheels\\dd\\c7\\f0\\a102782c25a396951685b12821797eaaad88e802e50271916f\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=df176b02c6506cadff1f1314a37a67cfecb09e10f59acbb623e76405037a4203\n",
      "  Stored in directory: c:\\users\\61431\\appdata\\local\\pip\\cache\\wheels\\6e\\9c\\ed\\4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, Pillow, cloudpickle, gym\n",
      "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 pyglet-1.5.0\n",
      "Collecting stable-baselines[mpi]\n",
      "  Using cached stable_baselines-2.10.1-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (1.19.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (1.0.1)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from stable-baselines[mpi]) (1.1.3)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-win_amd64.whl (34.9 MB)\n",
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.0.3-cp36-cp36m-win_amd64.whl (477 kB)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.2.0)\n",
      "Collecting atari-py~=0.2.0\n",
      "  Downloading atari_py-0.2.6-cp36-cp36m-win_amd64.whl (1.8 MB)\n",
      "Requirement already satisfied: six in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.15.0)\n",
      "Requirement already satisfied: future in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from pandas->stable-baselines[mpi]) (2021.1)\n",
      "Installing collected packages: opencv-python, kiwisolver, cycler, atari-py, matplotlib, stable-baselines, mpi4py\n",
      "Successfully installed atari-py-0.2.6 cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.4 mpi4py-3.0.3 opencv-python-4.5.1.48 stable-baselines-2.10.1\n",
      "Collecting tensorflow==1.14.0\n",
      "  Downloading tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3 MB)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "Collecting gast>=0.2.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.36.1-cp36-cp36m-win_amd64.whl (3.0 MB)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from tensorflow==1.14.0) (1.15.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from tensorflow==1.14.0) (1.19.2)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.15.6-cp36-cp36m-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from tensorflow==1.14.0) (0.36.2)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.3)\n",
      "Collecting cached-property\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\61431\\anaconda3\\envs\\rl new\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=853b332295b46a5f78fa3e6a8275ddf86e11c67b8d007187c8035b0719ef85eb\n",
      "  Stored in directory: c:\\users\\61431\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19553 sha256=4bde7cc973f2293daa0772ed883740aac3666028e992b36b317a0b6c03964eb9\n",
      "  Stored in directory: c:\\users\\61431\\appdata\\local\\pip\\cache\\wheels\\32\\42\\7f\\23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: cached-property, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 cached-property-1.5.2 gast-0.4.0 google-pasta-0.2.0 grpcio-1.36.1 h5py-3.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 protobuf-3.15.6 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install stable-baselines[mpi]\n",
    "!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\61431\\anaconda3\\envs\\RL New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class ATCEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, noOfPlanes, noOfRunways, planeToTypeMapping, runwayToTypeMapping, planeToStatusMapping):\n",
    "        super(ATCEnv, self).__init__()\n",
    "        \n",
    "        self.planeType = ['small','mid', 'jumbo']\n",
    "        self.runwayType = ['short', 'long', 'emergency']\n",
    "        self.status = ['normal', 'emergency']\n",
    "        \n",
    "        self.noOfPlanes = noOfPlanes\n",
    "        self.noOfRunways = noOfRunways\n",
    "       \n",
    "        self.totalNoOfPlanes = sum(noOfPlanes)\n",
    "        self.totalNoOfRunways = sum(noOfRunways)\n",
    "        \n",
    "        \n",
    "       \n",
    "        self.planeToTypeMapping = planeToTypeMapping\n",
    "        self.runwayToTypeMapping = runwayToTypeMapping\n",
    "        self.planeToStatusMapping = planeToStatusMapping\n",
    "     \n",
    "        \n",
    "        self.action_space = spaces.Tuple(\n",
    "                (spaces.Discrete(len(self.noOfRunways)), \n",
    "                (spaces.Discrete(len(self.noOfPlanes)))))\n",
    "                 \n",
    "        #self.observation_space = {}\n",
    "        \n",
    "        self.observation_space = {}\n",
    "        self.obs = {}\n",
    "        self.act = {}\n",
    "\n",
    "            \n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "            self.reward = 0\n",
    "            self.action_space = spaces.Tuple(\n",
    "                (spaces.Discrete(len(self.noOfRunways)), \n",
    "                (spaces.Discrete(len(self.noOfPlanes)))))\n",
    "            \n",
    "            self.observation_space ={}\n",
    "        \n",
    "       \n",
    "            self.current_step = 0\n",
    "            self.obs = {}\n",
    "            self.act = {}\n",
    "            \n",
    "            #self.noOfRunways = noOfRunways\n",
    "          \n",
    "            return self.observation_space\n",
    "            \n",
    "    def step(self, action):\n",
    "        runway = action[0]\n",
    "        plane = action[1]\n",
    "        \n",
    "        r = self.reward\n",
    "        \n",
    "        print(plane)\n",
    "        \n",
    "        if self.runwayToTypeMapping[runway] == self.runwayType[2] and self.planeToStatusMapping[plane] == self.status[1]:\n",
    "            self.reward = self.reward + 1\n",
    "        elif self.planeToStatusMapping[plane] != self.status[1] and self.runwayToTypeMapping[runway] not in ['short','emergency'] and self.planeToTypeMapping[plane] in ['mid', 'jumbo']:\n",
    "            self.reward = self.reward + 1\n",
    "        elif self.planeToStatusMapping[plane] != self.status[1] and self.runwayToTypeMapping[runway] == 'short' and self.planeToTypeMapping[plane] == 'small':\n",
    "            self.reward = self.reward + 1\n",
    "        else:\n",
    "            self.reward = self.reward - 1\n",
    "\n",
    "        self.current_step = self.current_step + 1\n",
    "        \n",
    "        if r <= self.reward:\n",
    "            self.observation_space[runway] = plane\n",
    "            obs = action\n",
    "            self.obs = obs\n",
    "        else:\n",
    "            obs = self.obs\n",
    "        act = action\n",
    "        \n",
    "        \n",
    "        #print(self.noOfRunways)\n",
    "        #print('runway',runway)\n",
    "        #self.noOfRunways.remove(runway)\n",
    "        \n",
    "        #print(self.noOfRunways)\n",
    "        \n",
    "        \n",
    "        \n",
    "        done = len(self.observation_space) == len(self.noOfRunways)\n",
    "        \n",
    "        print('o',self.observation_space)\n",
    "        print('r',self.noOfRunways)\n",
    "        print('reward', self.reward)\n",
    "        return act, obs, done\n",
    "\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "1\n",
      "o {}\n",
      "r [0, 1, 2, 3]\n",
      "reward -1\n",
      "(2, 1) {} False\n",
      "(2, 1)\n",
      "3\n",
      "o {0: 3}\n",
      "r [0, 1, 2, 3]\n",
      "reward 0\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward 1\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward 0\n",
      "(3, 3) (1, 0) False\n",
      "(3, 3)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -1\n",
      "(1, 3) (1, 0) False\n",
      "(1, 3)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -2\n",
      "(2, 3) (1, 0) False\n",
      "(2, 3)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -3\n",
      "(0, 1) (1, 0) False\n",
      "(0, 1)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -4\n",
      "(3, 1) (1, 0) False\n",
      "(3, 1)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -5\n",
      "(0, 0) (1, 0) False\n",
      "(0, 0)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -4\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -5\n",
      "(2, 1) (0, 3) False\n",
      "(2, 1)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -6\n",
      "(2, 0) (0, 3) False\n",
      "(2, 0)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -5\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -6\n",
      "(3, 3) (1, 0) False\n",
      "(3, 3)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -7\n",
      "(3, 0) (1, 0) False\n",
      "(3, 0)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(3, 0) (1, 0) False\n",
      "(3, 0)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(0, 1) (1, 0) False\n",
      "(0, 1)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(0, 1) (1, 0) False\n",
      "(0, 1)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "3\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(2, 3) (0, 3) False\n",
      "(2, 3)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "1\n",
      "o {0: 3, 1: 1}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(1, 1) (1, 1) False\n",
      "(1, 1)\n",
      "3\n",
      "o {0: 3, 1: 1}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(2, 3) (1, 1) False\n",
      "(2, 3)\n",
      "3\n",
      "o {0: 3, 1: 1}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "3\n",
      "o {0: 3, 1: 1}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(1, 3) (0, 3) False\n",
      "(1, 3)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "1\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(2, 1) (1, 0) False\n",
      "(2, 1)\n",
      "0\n",
      "o {0: 3, 1: 0}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "2\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -7\n",
      "(3, 2) (3, 2) False\n",
      "(3, 2)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -8\n",
      "(0, 0) (3, 2) False\n",
      "(0, 0)\n",
      "1\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(3, 1) (3, 2) False\n",
      "(3, 1)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(1, 3) (3, 2) False\n",
      "(1, 3)\n",
      "1\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -11\n",
      "(3, 1) (3, 2) False\n",
      "(3, 1)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(1, 1) (1, 1) False\n",
      "(1, 1)\n",
      "3\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(3, 1) (0, 3) False\n",
      "(3, 1)\n",
      "2\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(3, 2) (3, 2) False\n",
      "(3, 2)\n",
      "2\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(0, 2) (3, 2) False\n",
      "(0, 2)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -9\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "2\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -10\n",
      "(0, 2) (1, 0) False\n",
      "(0, 2)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -11\n",
      "(2, 0) (1, 0) False\n",
      "(2, 0)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -12\n",
      "(2, 3) (1, 0) False\n",
      "(2, 3)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -11\n",
      "(1, 0) (1, 0) False\n",
      "(1, 0)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -12\n",
      "(3, 0) (1, 0) False\n",
      "(3, 0)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -13\n",
      "(1, 3) (1, 0) False\n",
      "(1, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -14\n",
      "(1, 3) (1, 0) False\n",
      "(1, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -13\n",
      "(0, 3) (0, 3) False\n",
      "(0, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -14\n",
      "(2, 3) (0, 3) False\n",
      "(2, 3)\n",
      "2\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -15\n",
      "(0, 2) (0, 3) False\n",
      "(0, 2)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -16\n",
      "(2, 3) (0, 3) False\n",
      "(2, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -17\n",
      "(1, 3) (0, 3) False\n",
      "(1, 3)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -18\n",
      "(0, 0) (0, 3) False\n",
      "(0, 0)\n",
      "2\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -19\n",
      "(0, 2) (0, 3) False\n",
      "(0, 2)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -20\n",
      "(1, 3) (0, 3) False\n",
      "(1, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -21\n",
      "(2, 3) (0, 3) False\n",
      "(2, 3)\n",
      "3\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -22\n",
      "(3, 3) (0, 3) False\n",
      "(3, 3)\n",
      "0\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -23\n",
      "(2, 0) (0, 3) False\n",
      "(2, 0)\n",
      "2\n",
      "o {0: 3, 1: 0, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -24\n",
      "(0, 2) (0, 3) False\n",
      "(0, 2)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -23\n",
      "(1, 1) (1, 1) False\n",
      "(1, 1)\n",
      "2\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -24\n",
      "(0, 2) (1, 1) False\n",
      "(0, 2)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -23\n",
      "(1, 1) (1, 1) False\n",
      "(1, 1)\n",
      "0\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -24\n",
      "(0, 0) (1, 1) False\n",
      "(0, 0)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -25\n",
      "(3, 1) (1, 1) False\n",
      "(3, 1)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -26\n",
      "(0, 1) (1, 1) False\n",
      "(0, 1)\n",
      "2\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -25\n",
      "(3, 2) (3, 2) False\n",
      "(3, 2)\n",
      "0\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -26\n",
      "(2, 0) (3, 2) False\n",
      "(2, 0)\n",
      "0\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -27\n",
      "(0, 0) (3, 2) False\n",
      "(0, 0)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -28\n",
      "(0, 1) (3, 2) False\n",
      "(0, 1)\n",
      "0\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -29\n",
      "(3, 0) (3, 2) False\n",
      "(3, 0)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -30\n",
      "(0, 1) (3, 2) False\n",
      "(0, 1)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -31\n",
      "(0, 1) (3, 2) False\n",
      "(0, 1)\n",
      "1\n",
      "o {0: 3, 1: 1, 3: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -32\n",
      "(0, 1) (3, 2) False\n",
      "(0, 1)\n",
      "2\n",
      "o {0: 3, 1: 1, 3: 2, 2: 2}\n",
      "r [0, 1, 2, 3]\n",
      "reward -31\n",
      "(2, 2) (2, 2) True\n",
      "Finished after 73 timesteps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "noOfPlanes = [0,1,2,3]\n",
    "noOfRunways = [0,1,2,3] \n",
    "planeToTypeMapping = {0:'small',1:'small',2:'mid',3:'jumbo'}\n",
    "runwayToTypeMapping = {0:'emergency',1:'short',2:'long',3:'long'}\n",
    "planeToStatusMapping = {0:'normal',1:'normal',2:'normal',3:'emergency'}\n",
    "\n",
    "\n",
    "env = ATCEnv(noOfPlanes,noOfRunways,planeToTypeMapping,runwayToTypeMapping,planeToStatusMapping)\n",
    "#env = DummyVecEnv([lambda: ATCEnv(noOfPlanes,noOfRunways,planeToTypeMapping,runwayToTypeMapping,planeToStatusMapping)])\n",
    "\n",
    "#model = PPO2(MlpPolicy, env, verbose=1)\n",
    "#model.learn(total_timesteps=100)\n",
    "obs = env.reset()\n",
    "\n",
    "#obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "for t in range(100):\n",
    "        #env.render()\n",
    "        print(obs)\n",
    "        \n",
    "       # key = random.choice(list(env.action_space.keys()))\n",
    "       # val = env.action_space[key]\n",
    "        action = env.action_space.sample()\n",
    "        #action, _states = model.predict(obs)\n",
    "        obs, reward, done = env.step(action)\n",
    "        print(obs, reward, done)\n",
    "        if done:\n",
    "            print(\"Finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
